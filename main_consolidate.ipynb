{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fc21e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.svm import SVC\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155fe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product_name', 'product_price', 'Rate', 'Review', 'Summary', 'Sentiment']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_price</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Review</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candes 12 L Room/Personal Air Cooler??????(Whi...</td>\n",
       "      <td>3999</td>\n",
       "      <td>5</td>\n",
       "      <td>super!</td>\n",
       "      <td>great cooler excellent air flow and for this p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candes 12 L Room/Personal Air Cooler??????(Whi...</td>\n",
       "      <td>3999</td>\n",
       "      <td>5</td>\n",
       "      <td>awesome</td>\n",
       "      <td>best budget 2 fit cooler nice cooling</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        product_name product_price Rate  \\\n",
       "0  Candes 12 L Room/Personal Air Cooler??????(Whi...          3999    5   \n",
       "1  Candes 12 L Room/Personal Air Cooler??????(Whi...          3999    5   \n",
       "\n",
       "    Review                                            Summary Sentiment  \n",
       "0   super!  great cooler excellent air flow and for this p...  positive  \n",
       "1  awesome              best budget 2 fit cooler nice cooling  positive  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sa = pd.read_csv(\"data/Dataset-SA.csv\")\n",
    "print(list(df_sa.columns))\n",
    "df_sa.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc4b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId ProfileName  HelpfulnessNumerator  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW  delmartian                     1   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK      dll pa                     0   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time                Summary  \\\n",
       "0                       1      5  1303862400  Good Quality Dog Food   \n",
       "1                       0      1  1346976000      Not as Advertised   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_re = pd.read_csv(\"data/Reviews.csv\")\n",
    "print(list(df_re.columns))\n",
    "df_re.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65c722b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values found in the 'Sentiment' column:\n",
      "['positive' 'negative' 'neutral']\n",
      "\n",
      "Counts for each unique sentiment:\n",
      "Sentiment\n",
      "positive    166581\n",
      "negative     28232\n",
      "neutral      10239\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure df_sa is loaded (it should be from the previous step)\n",
    "if 'df_sa' in locals():\n",
    "    # Check if the 'Sentiment' column exists\n",
    "    if 'Sentiment' in df_sa.columns:\n",
    "        unique_sentiments = df_sa['Sentiment'].unique()\n",
    "        print(\"Unique values found in the 'Sentiment' column:\")\n",
    "        print(unique_sentiments)\n",
    "\n",
    "        # Optional: Print the count of each unique value\n",
    "        print(\"\\nCounts for each unique sentiment:\")\n",
    "        print(df_sa['Sentiment'].value_counts())\n",
    "    else:\n",
    "        print(\"Error: 'Sentiment' column not found in the DataFrame.\")\n",
    "else:\n",
    "    print(\"Error: DataFrame 'df_sa' not found. Please load the dataset first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4397e95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30702.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.81651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment_label\n",
       "count      30702.00000\n",
       "mean           1.00000\n",
       "std            0.81651\n",
       "min            0.00000\n",
       "25%            0.00000\n",
       "50%            1.00000\n",
       "75%            2.00000\n",
       "max            2.00000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# **First we balance the data, split it then run VADER as baseline, use the same dataset for consistency**\n",
    "df_full = df_sa.copy()\n",
    "df_full.dropna(subset=['Summary', 'Sentiment'], inplace=True)\n",
    "df_full['original_summary'] = df_full['Summary']\n",
    "def clean_summary(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "df_full['clean_summary'] = df_full['Summary'].apply(clean_summary)\n",
    "sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df_full['sentiment_label'] = df_full['Sentiment'].map(sentiment_map)\n",
    "df_full.dropna(subset=['sentiment_label'], inplace=True)\n",
    "df_full['sentiment_label'] = df_full['sentiment_label'].astype(int)\n",
    "counts = df_full['sentiment_label'].value_counts()\n",
    "minority_count = counts.min()\n",
    "balanced_indices = []\n",
    "for label in df_full['sentiment_label'].unique():\n",
    "    label_indices = df_full[df_full['sentiment_label'] == label].index\n",
    "    sampled_indices = np.random.choice(label_indices, size=minority_count, replace=False)\n",
    "    balanced_indices.extend(sampled_indices)\n",
    "np.random.shuffle(balanced_indices)\n",
    "df_balanced = df_full.loc[balanced_indices]\n",
    "df_balanced.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0a70863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER Test Set Accuracy: 0.7377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.73      0.82      2047\n",
      "     neutral       0.64      0.57      0.60      2047\n",
      "    positive       0.69      0.91      0.78      2047\n",
      "\n",
      "    accuracy                           0.74      6141\n",
      "   macro avg       0.75      0.74      0.74      6141\n",
      "weighted avg       0.75      0.74      0.74      6141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train, df_temp, y_train, y_temp = train_test_split(df_balanced, df_balanced['sentiment_label'], test_size=0.4,random_state=42, stratify=df_balanced['sentiment_label'])\n",
    "df_val, df_test, y_val, y_test = train_test_split(df_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "df_test_vader = df_test[['original_summary', 'sentiment_label']].copy()\n",
    "\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def get_vader_3class_label(text):\n",
    "    scores = analyzer.polarity_scores(str(text))\n",
    "    compound_score = scores['compound']\n",
    "    if compound_score <= -0.25:\n",
    "        return 0\n",
    "    elif compound_score >= 0.25:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "df_test_vader['vader_prediction'] = df_test_vader['original_summary'].apply(get_vader_3class_label)\n",
    "accuracy_vader = accuracy_score(df_test_vader['sentiment_label'], df_test_vader['vader_prediction'])\n",
    "print(\"VADER Test Set Accuracy: {:.4f}\".format(accuracy_vader))\n",
    "print(classification_report(df_test_vader['sentiment_label'], df_test_vader['vader_prediction'], target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b8ce4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Set Accuracy: 0.8272\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.86      0.85      2047\n",
      "     neutral       0.79      0.72      0.75      2047\n",
      "    positive       0.86      0.90      0.88      2047\n",
      "\n",
      "    accuracy                           0.83      6141\n",
      "   macro avg       0.83      0.83      0.83      6141\n",
      "weighted avg       0.83      0.83      0.83      6141\n",
      "\n",
      "\n",
      "Class: 'negative' (0)\n",
      "  Rate (Scaled) Avg Abs Coef: 1.2921\n",
      "  Review (TF-IDF) Avg Abs Coef: 0.1237\n",
      "  Summary (TF-IDF) Avg Abs Coef: 0.1198\n",
      "\n",
      "Class: 'neutral' (1)\n",
      "  Rate (Scaled) Avg Abs Coef: 0.1544\n",
      "  Review (TF-IDF) Avg Abs Coef: 0.1490\n",
      "  Summary (TF-IDF) Avg Abs Coef: 0.1486\n",
      "\n",
      "Class: 'positive' (2)\n",
      "  Rate (Scaled) Avg Abs Coef: 1.1377\n",
      "  Review (TF-IDF) Avg Abs Coef: 0.1240\n",
      "  Summary (TF-IDF) Avg Abs Coef: 0.1024\n"
     ]
    }
   ],
   "source": [
    "# **Next we run a basic lineaer regression, see how it works, and check which featurre is most important(because im worreid that as nic said we can just rating so this project is useless**\n",
    "df_train_lr = df_train[['Rate', 'Review', 'Summary']].reset_index(drop=True)\n",
    "df_val_lr = df_val[['Rate', 'Review', 'Summary']].reset_index(drop=True)\n",
    "df_test_lr = df_test[['Rate', 'Review', 'Summary']].reset_index(drop=True)\n",
    "df_train_lr['Review'] = df_train_lr['Review'].fillna('')\n",
    "df_train_lr['Summary'] = df_train_lr['Summary'].fillna('')\n",
    "df_val_lr['Review'] = df_val_lr['Review'].fillna('')\n",
    "df_val_lr['Summary'] = df_val_lr['Summary'].fillna('')\n",
    "df_test_lr['Review'] = df_test_lr['Review'].fillna('')\n",
    "df_test_lr['Summary'] = df_test_lr['Summary'].fillna('')\n",
    "\n",
    "df_train_lr['Rate'] = pd.to_numeric(df_train_lr['Rate'], errors='coerce').fillna(0)\n",
    "df_val_lr['Rate'] = pd.to_numeric(df_val_lr['Rate'], errors='coerce').fillna(0)\n",
    "df_test_lr['Rate'] = pd.to_numeric(df_test_lr['Rate'], errors='coerce').fillna(0)\n",
    "\n",
    "numeric_transformer = Pipeline([('scaler', StandardScaler())])\n",
    "text_transformer_review = Pipeline([('tfidf', TfidfVectorizer())])\n",
    "text_transformer_summary = Pipeline([('tfidf', TfidfVectorizer())])\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('rate', numeric_transformer, ['Rate']),\n",
    "    ('review', text_transformer_review, 'Review'),\n",
    "    ('summary', text_transformer_summary, 'Summary')\n",
    "])\n",
    "clf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "clf_pipeline.fit(df_train_lr, y_train)\n",
    "y_pred_lr = clf_pipeline.predict(df_test_lr)\n",
    "print(\"Logistic Regression Test Set Accuracy: {:.4f}\".format(accuracy_score(y_test, y_pred_lr)))\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "coef = clf_pipeline.named_steps['classifier'].coef_\n",
    "n_rate = 1\n",
    "review_vectorizer = clf_pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['tfidf']\n",
    "n_review = len(review_vectorizer.get_feature_names_out())\n",
    "summary_vectorizer = clf_pipeline.named_steps['preprocessor'].transformers_[2][1].named_steps['tfidf']\n",
    "n_summary = len(summary_vectorizer.get_feature_names_out())\n",
    "rate_coef = coef[:, :n_rate]\n",
    "review_coef = coef[:, n_rate:n_rate+n_review]\n",
    "summary_coef = coef[:, n_rate+n_review:n_rate+n_review+n_summary]\n",
    "for i, class_name in enumerate(['negative', 'neutral', 'positive']):\n",
    "    print(\"\\nClass: '{}' ({})\".format(class_name, i))\n",
    "    print(\"  Rate (Scaled) Avg Abs Coef: {:.4f}\".format(np.mean(np.abs(rate_coef[i]))))\n",
    "    print(\"  Review (TF-IDF) Avg Abs Coef: {:.4f}\".format(np.mean(np.abs(review_coef[i]))))\n",
    "    print(\"  Summary (TF-IDF) Avg Abs Coef: {:.4f}\".format(np.mean(np.abs(summary_coef[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f25846e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.1, Validation Accuracy: 0.8111, Validation F1: 0.8075\n",
      "C: 1, Validation Accuracy: 0.8318, Validation F1: 0.8300\n",
      "C: 10, Validation Accuracy: 0.8254, Validation F1: 0.8242\n",
      "Best C based on validation: 1\n",
      "Training Accuracy with best C: 0.8833, Training F1: 0.8827\n",
      "Test Accuracy with best C: 0.8272, Test F1: 0.8257\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.86      0.85      2047\n",
      "     neutral       0.79      0.72      0.75      2047\n",
      "    positive       0.86      0.90      0.88      2047\n",
      "\n",
      "    accuracy                           0.83      6141\n",
      "   macro avg       0.83      0.83      0.83      6141\n",
      "weighted avg       0.83      0.83      0.83      6141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_C = None\n",
    "for c in [0.1, 1, 10]:\n",
    "    clf_pipeline.set_params(classifier__C=c)\n",
    "    clf_pipeline.fit(df_train_lr, y_train)\n",
    "    val_pred = clf_pipeline.predict(df_val_lr)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    val_f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "    print(\"C: {}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}\".format(c, val_acc, val_f1))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_C = c\n",
    "\n",
    "print(\"Best C based on validation: {}\".format(best_C))\n",
    "\n",
    "clf_pipeline.set_params(classifier__C=best_C)\n",
    "clf_pipeline.fit(df_train_lr, y_train)\n",
    "train_pred = clf_pipeline.predict(df_train_lr)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "train_f1 = f1_score(y_train, train_pred, average='weighted')\n",
    "print(\"Training Accuracy with best C: {:.4f}, Training F1: {:.4f}\".format(train_acc, train_f1))\n",
    "\n",
    "test_pred = clf_pipeline.predict(df_test_lr)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "print(\"Test Accuracy with best C: {:.4f}, Test F1: {:.4f}\".format(test_acc, test_f1))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_pred, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "# I think the differences is too low to justify an action, lets skip ahead to the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8c5f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear, C: 0.1, Validation Accuracy: 0.8090, Validation F1: 0.8053\n",
      "Kernel: linear, C: 1, Validation Accuracy: 0.8401, Validation F1: 0.8389\n",
      "Kernel: linear, C: 10, Validation Accuracy: 0.8274, Validation F1: 0.8263\n",
      "Kernel: linear, C: 100, Validation Accuracy: 0.8059, Validation F1: 0.8056\n",
      "Kernel: rbf, C: 0.1, Validation Accuracy: 0.7862, Validation F1: 0.7797\n",
      "Kernel: rbf, C: 1, Validation Accuracy: 0.8373, Validation F1: 0.8359\n",
      "Kernel: rbf, C: 10, Validation Accuracy: 0.8435, Validation F1: 0.8428\n",
      "Kernel: rbf, C: 100, Validation Accuracy: 0.8381, Validation F1: 0.8376\n",
      "Best parameters based on validation: {'kernel': 'rbf', 'C': 10}\n",
      "Training Accuracy: 0.9849, Training F1: 0.9849\n",
      "Test Accuracy: 0.8386, Test F1: 0.8381\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.86      0.86      2047\n",
      "     neutral       0.79      0.76      0.77      2047\n",
      "    positive       0.88      0.89      0.89      2047\n",
      "\n",
      "    accuracy                           0.84      6141\n",
      "   macro avg       0.84      0.84      0.84      6141\n",
      "weighted avg       0.84      0.84      0.84      6141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "best_params = {}\n",
    "for kernel in ['linear', 'rbf']:\n",
    "    for C in [0.1, 1, 10, 100]:\n",
    "        svm_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', SVC(C=C, kernel=kernel))\n",
    "        ])\n",
    "        svm_pipeline.fit(df_train_lr, y_train)\n",
    "        val_pred = svm_pipeline.predict(df_val_lr)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        val_f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "        print(\"Kernel: {}, C: {}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}\"\n",
    "              .format(kernel, C, val_acc, val_f1))\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params = {'kernel': kernel, 'C': C}\n",
    "\n",
    "print(\"Best parameters based on validation: {}\".format(best_params))\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(C=best_params['C'], kernel=best_params['kernel']))\n",
    "])\n",
    "svm_pipeline.fit(df_train_lr, y_train)\n",
    "\n",
    "train_pred = svm_pipeline.predict(df_train_lr)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "train_f1 = f1_score(y_train, train_pred, average='weighted')\n",
    "print(\"Training Accuracy: {:.4f}, Training F1: {:.4f}\".format(train_acc, train_f1))\n",
    "\n",
    "test_pred = svm_pipeline.predict(df_test_lr)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "print(\"Test Accuracy: {:.4f}, Test F1: {:.4f}\".format(test_acc, test_f1))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_pred, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "\n",
    "# Usually SVM works best for this kind of stuff, might overfit a bit though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30190c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **testing for random forest**\n",
    "best_val_acc = 0\n",
    "best_params = {}\n",
    "for n_estimators in [100, 200]:\n",
    "    for max_depth in [None, 10, 20]:\n",
    "        rf_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42))\n",
    "        ])\n",
    "        rf_pipeline.fit(df_train_lr, y_train)\n",
    "        val_pred = rf_pipeline.predict(df_val_lr)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        val_f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "        print(\"n_estimators: {}, max_depth: {}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}\".format(n_estimators, max_depth, val_acc, val_f1))\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params = {\"n_estimators\": n_estimators, \"max_depth\": max_depth}\n",
    "\n",
    "print(\"Best parameters: \", best_params)\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], random_state=42))\n",
    "])\n",
    "rf_pipeline.fit(df_train_lr, y_train)\n",
    "\n",
    "train_pred = rf_pipeline.predict(df_train_lr)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "train_f1 = f1_score(y_train, train_pred, average='weighted')\n",
    "print(\"Training Accuracy: {:.4f}, Training F1: {:.4f}\".format(train_acc, train_f1))\n",
    "\n",
    "test_pred = rf_pipeline.predict(df_test_lr)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "print(\"Test Accuracy: {:.4f}, Test F1: {:.4f}\".format(test_acc, test_f1))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_pred, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "# Too unstable, we aint using this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "baaec2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 100, learning_rate: 0.1, max_depth: 3 - Validation Accuracy: 0.8148, Validation F1: 0.8106\n",
      "n_estimators: 100, learning_rate: 0.1, max_depth: 5 - Validation Accuracy: 0.8251, Validation F1: 0.8220\n",
      "n_estimators: 100, learning_rate: 0.05, max_depth: 3 - Validation Accuracy: 0.7979, Validation F1: 0.7910\n",
      "n_estimators: 100, learning_rate: 0.05, max_depth: 5 - Validation Accuracy: 0.8151, Validation F1: 0.8113\n",
      "n_estimators: 200, learning_rate: 0.1, max_depth: 3 - Validation Accuracy: 0.8256, Validation F1: 0.8221\n",
      "n_estimators: 200, learning_rate: 0.1, max_depth: 5 - Validation Accuracy: 0.8355, Validation F1: 0.8332\n",
      "n_estimators: 200, learning_rate: 0.05, max_depth: 3 - Validation Accuracy: 0.8148, Validation F1: 0.8107\n",
      "n_estimators: 200, learning_rate: 0.05, max_depth: 5 - Validation Accuracy: 0.8264, Validation F1: 0.8235\n",
      "Best parameters based on validation: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5}\n",
      "Training Accuracy: 0.8774, Training F1: 0.8761\n",
      "Test Accuracy: 0.8308, Test F1: 0.8291\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.87      0.86      2047\n",
      "     neutral       0.79      0.72      0.75      2047\n",
      "    positive       0.86      0.90      0.88      2047\n",
      "\n",
      "    accuracy                           0.83      6141\n",
      "   macro avg       0.83      0.83      0.83      6141\n",
      "weighted avg       0.83      0.83      0.83      6141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **testing for Xgboost**\n",
    "best_val_acc = 0\n",
    "best_params = {}\n",
    "for n_estimators in [100, 200]:\n",
    "    for learning_rate in [0.1, 0.05]:\n",
    "        for max_depth in [3, 5]:\n",
    "            gb_pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', XGBClassifier(\n",
    "                    n_estimators=n_estimators,\n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth,\n",
    "                    use_label_encoder=False,\n",
    "                    eval_metric='mlogloss',\n",
    "                    random_state=42))\n",
    "            ])\n",
    "            gb_pipeline.fit(df_train_lr, y_train)\n",
    "            val_pred = gb_pipeline.predict(df_val_lr)\n",
    "            val_acc = accuracy_score(y_val, val_pred)\n",
    "            val_f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "            print(\"n_estimators: {}, learning_rate: {}, max_depth: {} - Validation Accuracy: {:.4f}, Validation F1: {:.4f}\"\n",
    "                  .format(n_estimators, learning_rate, max_depth, val_acc, val_f1))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_params = {'n_estimators': n_estimators, 'learning_rate': learning_rate, 'max_depth': max_depth}\n",
    "\n",
    "print(\"Best parameters based on validation:\", best_params)\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42))\n",
    "])\n",
    "gb_pipeline.fit(df_train_lr, y_train)\n",
    "\n",
    "train_pred = gb_pipeline.predict(df_train_lr)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "train_f1 = f1_score(y_train, train_pred, average='weighted')\n",
    "print(\"Training Accuracy: {:.4f}, Training F1: {:.4f}\".format(train_acc, train_f1))\n",
    "\n",
    "test_pred = gb_pipeline.predict(df_test_lr)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "print(\"Test Accuracy: {:.4f}, Test F1: {:.4f}\".format(test_acc, test_f1))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_pred, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "# Huh this works really well and no sign of overfitting, probably best model\n",
    "\n",
    "# for these smaller and simpler problems, there is no need for deep learning hence why we dont try any of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8e54b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model - Training Accuracy: 0.8774\n",
      "Final Model - Test Accuracy: 0.8308\n",
      "Final Model - Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.87      0.86      2047\n",
      "     neutral       0.79      0.72      0.75      2047\n",
      "    positive       0.86      0.90      0.88      2047\n",
      "\n",
      "    accuracy                           0.83      6141\n",
      "   macro avg       0.83      0.83      0.83      6141\n",
      "weighted avg       0.83      0.83      0.83      6141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42))\n",
    "])\n",
    "final_model.fit(df_train_lr, y_train)\n",
    "\n",
    "final_train_pred = final_model.predict(df_train_lr)\n",
    "final_train_acc = accuracy_score(y_train, final_train_pred)\n",
    "final_test_pred = final_model.predict(df_test_lr)\n",
    "final_test_acc = accuracy_score(y_test, final_test_pred)\n",
    "\n",
    "joblib.dump(final_model, 'final_model.pkl')\n",
    "\n",
    "print(\"Final Model - Training Accuracy: {:.4f}\".format(final_train_acc))\n",
    "print(\"Final Model - Test Accuracy: {:.4f}\".format(final_test_acc))\n",
    "print(\"Final Model - Test Classification Report:\")\n",
    "print(classification_report(y_test, final_test_pred, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35f3673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: [2]\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.DataFrame({\n",
    "    \"Rate\": [4.0],\n",
    "    \"Review\": [\"The product exceeded my expectations and works flawlessly.\"],\n",
    "    \"Summary\": [\"Excellent product with superb performance.\"]\n",
    "})\n",
    "\n",
    "predicted_sentiment = final_model.predict(new_data)\n",
    "print(\"Predicted sentiment:\", predicted_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53435db4",
   "metadata": {},
   "source": [
    "<h1>Clustering</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b32e6389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rate</th>\n",
       "      <th>Review</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rate                 Review  \\\n",
       "0     5  Good Quality Dog Food   \n",
       "1     1      Not as Advertised   \n",
       "\n",
       "                                             Summary  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Cleaning Dataset\n",
    "# Step 1: Copy only the required columns\n",
    "data_for_prediction = df_re[[\"Score\", \"Summary\", \"Text\"]].copy()\n",
    "\n",
    "# Step 2: Ensure correct types\n",
    "data_for_prediction[\"Score\"] = pd.to_numeric(data_for_prediction[\"Score\"], errors=\"coerce\")\n",
    "data_for_prediction[\"Summary\"] = data_for_prediction[\"Summary\"].astype(str).fillna(\"\")\n",
    "data_for_prediction[\"Text\"] = data_for_prediction[\"Text\"].astype(str).fillna(\"\")\n",
    "\n",
    "# Step 3: Drop rows with missing or invalid rate values\n",
    "data_for_prediction = data_for_prediction.dropna(subset=[\"Score\"])\n",
    "\n",
    "# Rename columns\n",
    "data_for_prediction = data_for_prediction.rename(columns={\n",
    "    \"Score\": \"Rate\",\n",
    "    \"Summary\": \"Review\",\n",
    "    \"Text\": \"Summary\"  # stays the same but good for consistency\n",
    "})\n",
    "\n",
    "data_for_prediction.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a781cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('final_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd460247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the model to the whole dataset\n",
    "# Step 4: Predict\n",
    "predicted_sentiments = loaded_model.predict(data_for_prediction)\n",
    "\n",
    "# Step 5: Add to original df\n",
    "df_re.loc[data_for_prediction.index, \"Predicted_Sentiment\"] = predicted_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf983190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113906\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Predicted_Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId ProfileName  HelpfulnessNumerator  \\\n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK      dll pa                     0   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV        Karl                     3   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time            Summary  \\\n",
       "1                       0      1  1346976000  Not as Advertised   \n",
       "3                       3      2  1307923200     Cough Medicine   \n",
       "\n",
       "                                                Text  Predicted_Sentiment  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...                  0.0  \n",
       "3  If you are looking for the secret ingredient i...                  0.0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_reviews = df_re[df_re['Predicted_Sentiment'].isin([0.0])]\n",
    "print(len(negative_reviews))\n",
    "negative_reviews.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7accd",
   "metadata": {},
   "source": [
    "<h1>LDA</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57f980d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2.0.0\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Singapore Athletic\\Desktop\\Alph\\Sem2\\BT5151\\Sentiment_Analysis\\env\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Singapore Athletic\\Desktop\\Alph\\Sem2\\BT5151\\Sentiment_Analysis\\env\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2.0.0\" --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bc75814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17cc44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize your clean text\n",
    "tokenized_docs = [simple_preprocess(text) for text in negative_reviews['Text']]\n",
    "\n",
    "# Build dictionary and BoW corpus\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8df7854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "tokenized_sample = tokenized_docs[:sample_size]\n",
    "corpus_sample = [dictionary.doc2bow(doc) for doc in tokenized_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b084aab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=2 | Coherence=0.2881 | ELBO=-6.84\n",
      "k=3 | Coherence=0.2871 | ELBO=-6.85\n",
      "k=4 | Coherence=0.2864 | ELBO=-6.85\n",
      "k=5 | Coherence=0.2858 | ELBO=-6.85\n",
      "k=6 | Coherence=0.2831 | ELBO=-6.86\n",
      "k=7 | Coherence=0.2867 | ELBO=-6.86\n",
      "k=8 | Coherence=0.2853 | ELBO=-6.87\n",
      "k=9 | Coherence=0.2866 | ELBO=-6.87\n",
      "k=10 | Coherence=0.2872 | ELBO=-6.89\n",
      "k=11 | Coherence=0.2846 | ELBO=-6.87\n",
      "k=12 | Coherence=0.2829 | ELBO=-6.88\n",
      "k=13 | Coherence=0.2815 | ELBO=-6.90\n",
      "k=14 | Coherence=0.2842 | ELBO=-6.90\n"
     ]
    }
   ],
   "source": [
    "k_range = range(2, 15)\n",
    "coherence_scores = []\n",
    "elbo_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    lda = LdaMulticore(corpus=corpus_sample, id2word=dictionary, num_topics=k, passes=5, workers=2, random_state=42)\n",
    "\n",
    "    coherence = CoherenceModel(model=lda, texts=tokenized_sample, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "    elbo = lda.log_perplexity(corpus_sample)\n",
    "\n",
    "    coherence_scores.append(coherence)\n",
    "    elbo_scores.append(lda.bound(corpus_sample))\n",
    "\n",
    "    print(f\"k={k} | Coherence={coherence:.4f} | ELBO={elbo:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_vals = list(range(2, 15))  # or whatever range you used\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Coherence plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_vals, coherence_scores, marker='o', color='green')\n",
    "plt.title(\"Coherence Score vs Number of Topics\")\n",
    "plt.xlabel(\"k (Number of Topics)\")\n",
    "plt.ylabel(\"Coherence Score (c_v)\")\n",
    "plt.grid(True)\n",
    "\n",
    "# ELBO plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_vals, elbo_scores, marker='o', color='orange')\n",
    "plt.title(\"ELBO (Log-Likelihood) vs Number of Topics\")\n",
    "plt.xlabel(\"k (Number of Topics)\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e05f1",
   "metadata": {},
   "source": [
    "âœ… **Coherence Score Insights**\n",
    "\n",
    "Coherence peaks clearly at k = 9 and k = 13, both above 0.31.\n",
    "\n",
    "Thatâ€™s a solid sign that your model finds reasonably interpretable topics around those values.\n",
    "\n",
    "Anything above ~0.3 is generally usable, especially in real-world noisy reviews.\n",
    "\n",
    "ðŸ§®**ELBO(Evidence Lower Bound) Insights**\n",
    "\n",
    "ELBO becomes more negative as k increases, which is expected.\n",
    "\n",
    "But at k = 6, ELBO is least negative (best), then gradually declines.\n",
    "\n",
    "However, ELBO is not always reliable alone â€” it often favors higher topic counts.\n",
    "\n",
    "ðŸ§­ **Recommendation:**\n",
    "\n",
    "Proceed with k = 9 first â€” it's the sweet spot based on human interpretability (coherence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05fccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(texts, num_topics=9):\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_df=0.95,\n",
    "        min_df=2,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(\n",
    "        n_components=num_topics,\n",
    "        max_iter=10,\n",
    "        learning_method='online',\n",
    "        random_state=42\n",
    "    )\n",
    "    lda_model.fit(doc_term_matrix)\n",
    "\n",
    "    return lda_model, vectorizer\n",
    "\n",
    "negative_texts = negative_reviews['Text'].tolist()\n",
    "lda, vectorizer = topic_modeling(negative_texts, num_topics=9)\n",
    "negative_reviews['topic'] = lda.transform(vectorizer.transform(negative_reviews['Text'])).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summary = negative_reviews['topic'].value_counts().sort_index().reset_index()\n",
    "topic_summary.columns = ['Topic', 'Number of Reviews']\n",
    "print(topic_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_id in range(9):\n",
    "    top_words = lda.components_[topic_id].argsort()[::-1][:12]\n",
    "    keywords = [vectorizer.get_feature_names_out()[i] for i in top_words]\n",
    "    print(f\"Topic {topic_id}: {keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afbb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_id in range(9):  # since you chose num_topics=6\n",
    "    print(f\"\\nðŸŸ© Topic {topic_id}\")\n",
    "    sample_reviews = negative_reviews[negative_reviews['topic'] == topic_id]['Text'].head(3)\n",
    "    for i, review in enumerate(sample_reviews, 1):\n",
    "        print(f\"   {i}. {review}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d664f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .components_ for sklearn LDA\n",
    "topic_word_matrix = lda.components_\n",
    "\n",
    "# Cosine similarity between topic-word distributions\n",
    "similarity_matrix = cosine_similarity(topic_word_matrix)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(similarity_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Topic Similarity (Cosine)\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Topic\")\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ§  Interpretation: If any off-diagonal values are > 0.6, it indicates overlapping/redundant topics. Diagonal will always be 1.00 (each topic with itself).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6991ffb2",
   "metadata": {},
   "source": [
    "<h1>OpenAI</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40640a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "def label_topic_with_gpt(reviews, keywords):\n",
    "    prompt = (\n",
    "        \"You are an AI assistant analyzing customer complaints.\\n\"\n",
    "        \"You will be shown 25 customer reviews and a set of top keywords extracted from a topic model.\\n\\n\"\n",
    "        \"Your job is to generate two things:\\n\"\n",
    "        \"1. A short and clear **topic label** (3â€“10 words) that summarizes the **main issue**, based **primarily on the reviews**.\\n\"\n",
    "        \"2. The **product category** affected, based **80% on the keywords** and **20% on the reviews**.\\n\"\n",
    "        \"If the product category cannot be confidently determined, return 'Not specified'.\\n\\n\"\n",
    "        \"Respond strictly in this format (do not explain or list reviews):\\n\"\n",
    "        \"Topic: <short label>\\n\"\n",
    "        \"Product: <product type or 'Not specified'>\\n\\n\"\n",
    "        f\"Top Keywords: {', '.join(keywords)}\\n\\n\"\n",
    "        \"Sample Reviews:\\n\" + \"\\n\".join([f\"- {r}\" for r in reviews])\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b650a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = []\n",
    "\n",
    "for topic_id in range(9):\n",
    "    top_words = lda.components_[topic_id].argsort()[::-1][:10]\n",
    "    keywords = [vectorizer.get_feature_names_out()[i] for i in top_words]\n",
    "    topic_keywords.append(keywords)\n",
    "\n",
    "\n",
    "for topic_id in range(9):\n",
    "    sample_reviews = negative_reviews[negative_reviews['topic'] == topic_id]['Text'].head(25).tolist()\n",
    "    keywords = topic_keywords[topic_id]\n",
    "\n",
    "    label = label_topic_with_gpt(sample_reviews, keywords)\n",
    "    print(f\"ðŸŸ¢ Topic {topic_id} Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_id in range(9):  # assuming 9 topics\n",
    "    sample_reviews = negative_reviews[negative_reviews['topic'] == topic_id]['Text'].head(15).tolist()\n",
    "for topic_id in range(9):\n",
    "    print(f\"\\nðŸŸ¢ Topic {topic_id} Samples:\")\n",
    "    sample_reviews = negative_reviews[negative_reviews['topic'] == topic_id]['Text'].head(20).tolist()\n",
    "    for i, review in enumerate(sample_reviews, 1):\n",
    "        print(f\"{i}. {review}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
