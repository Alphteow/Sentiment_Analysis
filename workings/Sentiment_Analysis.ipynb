{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e040968-90ff-42bf-b300-fc9ff52af56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Dataset-SA.csv:\n",
      "['product_name', 'product_price', 'Rate', 'Review', 'Summary', 'Sentiment']\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "                                        product_name product_price Rate  \\\n",
      "0  Candes 12 L Room/Personal Air Cooler??????(Whi...          3999    5   \n",
      "1  Candes 12 L Room/Personal Air Cooler??????(Whi...          3999    5   \n",
      "2  Candes 12 L Room/Personal Air Cooler??????(Whi...          3999    3   \n",
      "3  Candes 12 L Room/Personal Air Cooler??????(Whi...          3999    1   \n",
      "4  Candes 12 L Room/Personal Air Cooler??????(Whi...          3999    3   \n",
      "\n",
      "            Review                                            Summary  \\\n",
      "0           super!  great cooler excellent air flow and for this p...   \n",
      "1          awesome              best budget 2 fit cooler nice cooling   \n",
      "2             fair  the quality is good but the power of air is de...   \n",
      "3  useless product                  very bad product its a only a fan   \n",
      "4             fair                                      ok ok product   \n",
      "\n",
      "  Sentiment  \n",
      "0  positive  \n",
      "1  positive  \n",
      "2  positive  \n",
      "3  negative  \n",
      "4   neutral  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"Dataset-SA.csv\"\n",
    "\n",
    "\n",
    "df_sa = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Columns in Dataset-SA.csv:\")\n",
    "print(list(df_sa.columns))\n",
    "\n",
    "\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df_sa.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8afaf94f-414f-43e8-bb21-2c4aebc21613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in review.csv:\n",
      "['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text']\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file_path = \"Reviews.csv\"\n",
    "\n",
    "\n",
    "df_re = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Columns in review.csv:\")\n",
    "print(list(df_re.columns))\n",
    "\n",
    "\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df_re.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93ba24dd-7d69-4316-b3d4-95eb181cafc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values found in the 'Sentiment' column:\n",
      "['positive' 'negative' 'neutral']\n",
      "\n",
      "Counts for each unique sentiment:\n",
      "Sentiment\n",
      "positive    166581\n",
      "negative     28232\n",
      "neutral      10239\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure df_sa is loaded (it should be from the previous step)\n",
    "if 'df_sa' in locals():\n",
    "    # Check if the 'Sentiment' column exists\n",
    "    if 'Sentiment' in df_sa.columns:\n",
    "        unique_sentiments = df_sa['Sentiment'].unique()\n",
    "        print(\"Unique values found in the 'Sentiment' column:\")\n",
    "        print(unique_sentiments)\n",
    "\n",
    "        # Optional: Print the count of each unique value\n",
    "        print(\"\\nCounts for each unique sentiment:\")\n",
    "        print(df_sa['Sentiment'].value_counts())\n",
    "    else:\n",
    "        print(\"Error: 'Sentiment' column not found in the DataFrame.\")\n",
    "else:\n",
    "    print(\"Error: DataFrame 'df_sa' not found. Please load the dataset first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47718a-b403-45bc-985f-613b0fa0724c",
   "metadata": {},
   "source": [
    "# **First we balance the data, split it then run VADER as baseline, use the same dataset for consistency**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf4e850a-129e-4b58-9069-5716a02889b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER Test Set Accuracy: 0.7541\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.73      0.83      2047\n",
      "     neutral       0.66      0.61      0.63      2047\n",
      "    positive       0.71      0.92      0.80      2047\n",
      "\n",
      "    accuracy                           0.75      6141\n",
      "   macro avg       0.77      0.75      0.75      6141\n",
      "weighted avg       0.77      0.75      0.75      6141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "df_full = df_sa.copy()\n",
    "df_full.dropna(subset=['Summary', 'Sentiment'], inplace=True)\n",
    "df_full['original_summary'] = df_full['Summary']\n",
    "def clean_summary(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "df_full['clean_summary'] = df_full['Summary'].apply(clean_summary)\n",
    "sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df_full['sentiment_label'] = df_full['Sentiment'].map(sentiment_map)\n",
    "df_full.dropna(subset=['sentiment_label'], inplace=True)\n",
    "df_full['sentiment_label'] = df_full['sentiment_label'].astype(int)\n",
    "counts = df_full['sentiment_label'].value_counts()\n",
    "minority_count = counts.min()\n",
    "balanced_indices = []\n",
    "for label in df_full['sentiment_label'].unique():\n",
    "    label_indices = df_full[df_full['sentiment_label'] == label].index\n",
    "    sampled_indices = np.random.choice(label_indices, size=minority_count, replace=False)\n",
    "    balanced_indices.extend(sampled_indices)\n",
    "np.random.shuffle(balanced_indices)\n",
    "df_balanced = df_full.loc[balanced_indices]\n",
    "\n",
    "df_train, df_temp, y_train, y_temp = train_test_split(df_balanced, df_balanced['sentiment_label'], test_size=0.4,random_state=42, stratify=df_balanced['sentiment_label'])\n",
    "df_val, df_test, y_val, y_test = train_test_split(df_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "df_test_vader = df_test[['original_summary', 'sentiment_label']].copy()\n",
    "\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def get_vader_3class_label(text):\n",
    "    scores = analyzer.polarity_scores(str(text))\n",
    "    compound_score = scores['compound']\n",
    "    if compound_score <= -0.25:\n",
    "        return 0\n",
    "    elif compound_score >= 0.25:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "df_test_vader['vader_prediction'] = df_test_vader['original_summary'].apply(get_vader_3class_label)\n",
    "accuracy_vader = accuracy_score(df_test_vader['sentiment_label'], df_test_vader['vader_prediction'])\n",
    "print(\"VADER Test Set Accuracy: {:.4f}\".format(accuracy_vader))\n",
    "print(classification_report(df_test_vader['sentiment_label'], df_test_vader['vader_prediction'], target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d5c0b-5356-4241-b64a-a5703dfcb0bd",
   "metadata": {},
   "source": [
    "# **Next we run a basic lineaer regression, see how it works, and check which featurre is most important(because im worreid that as nic said we can just rating so this project is useless**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e741e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn components\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3453be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Set Accuracy: 0.8346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.87      0.85      2047\n",
      "     neutral       0.79      0.74      0.76      2047\n",
      "    positive       0.87      0.90      0.88      2047\n",
      "\n",
      "    accuracy                           0.83      6141\n",
      "   macro avg       0.83      0.83      0.83      6141\n",
      "weighted avg       0.83      0.83      0.83      6141\n",
      "\n",
      "\n",
      "Class: 'negative' (0)\n",
      "  Rate (Scaled) Avg Abs Coef: 1.2418\n",
      "  Review (TF-IDF) Avg Abs Coef: 0.1481\n",
      "  Summary (TF-IDF) Avg Abs Coef: 0.1230\n",
      "\n",
      "Class: 'neutral' (1)\n",
      "  Rate (Scaled) Avg Abs Coef: 0.0980\n",
      "  Review (TF-IDF) Avg Abs Coef: 0.1495\n",
      "  Summary (TF-IDF) Avg Abs Coef: 0.1513\n",
      "\n",
      "Class: 'positive' (2)\n",
      "  Rate (Scaled) Avg Abs Coef: 1.1438\n",
      "  Review (TF-IDF) Avg Abs Coef: 0.1443\n",
      "  Summary (TF-IDF) Avg Abs Coef: 0.1057\n"
     ]
    }
   ],
   "source": [
    "df_train_lr = df_train[['Rate', 'Review', 'Summary']].reset_index(drop=True)\n",
    "df_test_lr = df_test[['Rate', 'Review', 'Summary']].reset_index(drop=True)\n",
    "df_train_lr['Review'] = df_train_lr['Review'].fillna('')\n",
    "df_train_lr['Summary'] = df_train_lr['Summary'].fillna('')\n",
    "df_test_lr['Review'] = df_test_lr['Review'].fillna('')\n",
    "df_test_lr['Summary'] = df_test_lr['Summary'].fillna('')\n",
    "\n",
    "# Convert the \"Rate\" column to numeric, coercing errors to NaN then filling with 0\n",
    "# df_train_lr['Rate'] = pd.to_numeric(df_train_lr['Rate'], errors='coerce').fillna(0)\n",
    "# df_test_lr['Rate'] = pd.to_numeric(df_test_lr['Rate'], errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "df_train_lr['Rate'] = pd.to_numeric(df_train_lr['Rate'], errors='coerce').fillna(0)\n",
    "df_val_lr['Rate'] = pd.to_numeric(df_val_lr['Rate'], errors='coerce').fillna(0)\n",
    "df_test_lr['Rate'] = pd.to_numeric(df_test_lr['Rate'], errors='coerce').fillna(0)\n",
    "\n",
    "numeric_transformer = Pipeline([('scaler', StandardScaler())])\n",
    "text_transformer_review = Pipeline([('tfidf', TfidfVectorizer())])\n",
    "text_transformer_summary = Pipeline([('tfidf', TfidfVectorizer())])\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('rate', numeric_transformer, ['Rate']),\n",
    "    ('review', text_transformer_review, 'Review'),\n",
    "    ('summary', text_transformer_summary, 'Summary')\n",
    "])\n",
    "clf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "clf_pipeline.fit(df_train_lr, y_train)\n",
    "y_pred_lr = clf_pipeline.predict(df_test_lr)\n",
    "print(\"Logistic Regression Test Set Accuracy: {:.4f}\".format(accuracy_score(y_test, y_pred_lr)))\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "coef = clf_pipeline.named_steps['classifier'].coef_\n",
    "n_rate = 1\n",
    "review_vectorizer = clf_pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['tfidf']\n",
    "n_review = len(review_vectorizer.get_feature_names_out())\n",
    "summary_vectorizer = clf_pipeline.named_steps['preprocessor'].transformers_[2][1].named_steps['tfidf']\n",
    "n_summary = len(summary_vectorizer.get_feature_names_out())\n",
    "rate_coef = coef[:, :n_rate]\n",
    "review_coef = coef[:, n_rate:n_rate+n_review]\n",
    "summary_coef = coef[:, n_rate+n_review:n_rate+n_review+n_summary]\n",
    "for i, class_name in enumerate(['negative', 'neutral', 'positive']):\n",
    "    print(\"\\nClass: '{}' ({})\".format(class_name, i))\n",
    "    print(\"  Rate (Scaled) Avg Abs Coef: {:.4f}\".format(np.mean(np.abs(rate_coef[i]))))\n",
    "    print(\"  Review (TF-IDF) Avg Abs Coef: {:.4f}\".format(np.mean(np.abs(review_coef[i]))))\n",
    "    print(\"  Summary (TF-IDF) Avg Abs Coef: {:.4f}\".format(np.mean(np.abs(summary_coef[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84765c0c-7964-4970-ace6-e2f3c1429756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_lr = df_train[['Rate', 'Review', 'Summary']].reset_index(drop=True)\n",
    "# df_test_lr = df_test[['Rate', 'Review', 'Summary']].reset_index(drop=True)\n",
    "# df_train_lr['Review'] = df_train_lr['Review'].fillna('')\n",
    "# df_train_lr['Summary'] = df_train_lr['Summary'].fillna('')\n",
    "# df_test_lr['Review'] = df_test_lr['Review'].fillna('')\n",
    "# df_test_lr['Summary'] = df_test_lr['Summary'].fillna('')\n",
    "\n",
    "# numeric_transformer = Pipeline([('scaler', StandardScaler())])\n",
    "# text_transformer_review = Pipeline([('tfidf', TfidfVectorizer())])\n",
    "# text_transformer_summary = Pipeline([('tfidf', TfidfVectorizer())])\n",
    "# preprocessor = ColumnTransformer(transformers=[\n",
    "#     ('rate', numeric_transformer, ['Rate']),\n",
    "#     ('review', text_transformer_review, 'Review'),\n",
    "#     ('summary', text_transformer_summary, 'Summary')\n",
    "# ])\n",
    "# clf_pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', LogisticRegression(max_iter=1000))\n",
    "# ])\n",
    "# clf_pipeline.fit(df_train_lr, y_train)\n",
    "# y_pred_lr = clf_pipeline.predict(df_test_lr)\n",
    "# print(\"Logistic Regression Test Set Accuracy: {:.4f}\".format(accuracy_score(y_test, y_pred_lr)))\n",
    "# print(classification_report(y_test, y_pred_lr, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "# coef = clf_pipeline.named_steps['classifier'].coef_\n",
    "# n_rate = 1\n",
    "# review_vectorizer = clf_pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['tfidf']\n",
    "# n_review = len(review_vectorizer.get_feature_names_out())\n",
    "# summary_vectorizer = clf_pipeline.named_steps['preprocessor'].transformers_[2][1].named_steps['tfidf']\n",
    "# n_summary = len(summary_vectorizer.get_feature_names_out())\n",
    "# rate_coef = coef[:, :n_rate]\n",
    "# review_coef = coef[:, n_rate:n_rate+n_review]\n",
    "# summary_coef = coef[:, n_rate+n_review:n_rate+n_review+n_summary]\n",
    "# for i, class_name in enumerate(['negative', 'neutral', 'positive']):\n",
    "#     print(\"\\nClass: '{}' ({})\".format(class_name, i))\n",
    "#     print(\"  Rate (Scaled) Avg Abs Coef: {:.4f}\".format(np.mean(np.abs(rate_coef[i]))))\n",
    "#     print(\"  Review (TF-IDF) Avg Abs Coef: {:.4f}\".format(np.mean(np.abs(review_coef[i]))))\n",
    "#     print(\"  Summary (TF-IDF) Avg Abs Coef: {:.4f}\".format(np.mean(np.abs(summary_coef[i]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b417af6-b398-4035-a095-a00ef948fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #whoops forgor to define val previously\n",
    "\n",
    "# df_train_lr = df_train[['Rate', 'Review', 'Summary']].reset_index(drop=True)\n",
    "# df_val_lr = df_val[['Rate', 'Review', 'Summary']].reset_index(drop=True)\n",
    "# df_test_lr = df_test[['Rate', 'Review', 'Summary']].reset_index(drop=True)\n",
    "# df_train_lr['Review'] = df_train_lr['Review'].fillna('')\n",
    "# df_train_lr['Summary'] = df_train_lr['Summary'].fillna('')\n",
    "# df_val_lr['Review'] = df_val_lr['Review'].fillna('')\n",
    "# df_val_lr['Summary'] = df_val_lr['Summary'].fillna('')\n",
    "# df_test_lr['Review'] = df_test_lr['Review'].fillna('')\n",
    "# df_test_lr['Summary'] = df_test_lr['Summary'].fillna('')\n",
    "\n",
    "# best_acc = 0\n",
    "# best_C = None\n",
    "# for c in [0.1, 1, 10]:\n",
    "#     clf_pipeline.set_params(classifier__C=c)\n",
    "#     clf_pipeline.fit(df_train_lr, y_train)\n",
    "#     val_pred = clf_pipeline.predict(df_val_lr)\n",
    "#     val_acc = accuracy_score(y_val, val_pred)\n",
    "#     print(\"C: {}, Validation Accuracy: {:.4f}\".format(c, val_acc))\n",
    "#     if val_acc > best_acc:\n",
    "#         best_acc = val_acc\n",
    "#         best_C = c\n",
    "\n",
    "# print(\"Best C based on validation: {}\".format(best_C))\n",
    "\n",
    "# clf_pipeline.set_params(classifier__C=best_C)\n",
    "# clf_pipeline.fit(df_train_lr, y_train)\n",
    "# train_pred = clf_pipeline.predict(df_train_lr)\n",
    "# train_acc = accuracy_score(y_train, train_pred)\n",
    "# print(\"Training Accuracy with best C: {:.4f}\".format(train_acc))\n",
    "# test_pred = clf_pipeline.predict(df_test_lr)\n",
    "# test_acc = accuracy_score(y_test, test_pred)\n",
    "# print(\"Test Accuracy with best C: {:.4f}\".format(test_acc))\n",
    "# print(\"Test Classification Report:\")\n",
    "# print(classification_report(y_test, test_pred, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d6eb822-3e16-465a-a6ec-80f725443413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.1, Validation Accuracy: 0.8178, Validation F1: 0.8148\n",
      "C: 1, Validation Accuracy: 0.8529, Validation F1: 0.8523\n",
      "C: 10, Validation Accuracy: 0.8647, Validation F1: 0.8649\n",
      "Best C based on validation: 10\n",
      "Training Accuracy with best C: 0.9304, Training F1: 0.9301\n",
      "Test Accuracy with best C: 0.8271, Test F1: 0.8266\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.85      0.85      2047\n",
      "     neutral       0.77      0.75      0.76      2047\n",
      "    positive       0.87      0.88      0.87      2047\n",
      "\n",
      "    accuracy                           0.83      6141\n",
      "   macro avg       0.83      0.83      0.83      6141\n",
      "weighted avg       0.83      0.83      0.83      6141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_acc = 0\n",
    "best_C = None\n",
    "for c in [0.1, 1, 10]:\n",
    "    clf_pipeline.set_params(classifier__C=c)\n",
    "    clf_pipeline.fit(df_train_lr, y_train)\n",
    "    val_pred = clf_pipeline.predict(df_val_lr)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    val_f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "    print(\"C: {}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}\".format(c, val_acc, val_f1))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_C = c\n",
    "\n",
    "print(\"Best C based on validation: {}\".format(best_C))\n",
    "\n",
    "clf_pipeline.set_params(classifier__C=best_C)\n",
    "clf_pipeline.fit(df_train_lr, y_train)\n",
    "train_pred = clf_pipeline.predict(df_train_lr)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "train_f1 = f1_score(y_train, train_pred, average='weighted')\n",
    "print(\"Training Accuracy with best C: {:.4f}, Training F1: {:.4f}\".format(train_acc, train_f1))\n",
    "\n",
    "test_pred = clf_pipeline.predict(df_test_lr)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "print(\"Test Accuracy with best C: {:.4f}, Test F1: {:.4f}\".format(test_acc, test_f1))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_pred, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3c9ce-ed61-40d4-af9c-e6f7292cdc59",
   "metadata": {},
   "source": [
    "I think the differences is too low to justify an action, lets skip ahead to the second model\n",
    "\n",
    "# **testing for SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5489da8d-adef-4566-9786-f44a5362dba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear, C: 0.1, Validation Accuracy: 0.8129, Validation F1: 0.8099\n",
      "Kernel: linear, C: 1, Validation Accuracy: 0.8596, Validation F1: 0.8596\n",
      "Kernel: linear, C: 10, Validation Accuracy: 0.8630, Validation F1: 0.8634\n",
      "Kernel: linear, C: 100, Validation Accuracy: 0.8533, Validation F1: 0.8542\n",
      "Kernel: rbf, C: 0.1, Validation Accuracy: 0.7839, Validation F1: 0.7768\n",
      "Kernel: rbf, C: 1, Validation Accuracy: 0.8586, Validation F1: 0.8585\n",
      "Kernel: rbf, C: 10, Validation Accuracy: 0.8914, Validation F1: 0.8920\n",
      "Kernel: rbf, C: 100, Validation Accuracy: 0.8852, Validation F1: 0.8860\n",
      "Best parameters based on validation: {'kernel': 'rbf', 'C': 10}\n",
      "Training Accuracy: 0.9845, Training F1: 0.9845\n",
      "Test Accuracy: 0.8476, Test F1: 0.8474\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.87      0.87      2047\n",
      "     neutral       0.79      0.78      0.79      2047\n",
      "    positive       0.89      0.89      0.89      2047\n",
      "\n",
      "    accuracy                           0.85      6141\n",
      "   macro avg       0.85      0.85      0.85      6141\n",
      "weighted avg       0.85      0.85      0.85      6141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_val_acc = 0\n",
    "best_params = {}\n",
    "for kernel in ['linear', 'rbf']:\n",
    "    for C in [0.1, 1, 10, 100]:\n",
    "        svm_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', SVC(C=C, kernel=kernel))\n",
    "        ])\n",
    "        svm_pipeline.fit(df_train_lr, y_train)\n",
    "        val_pred = svm_pipeline.predict(df_val_lr)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        val_f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "        print(\"Kernel: {}, C: {}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}\"\n",
    "              .format(kernel, C, val_acc, val_f1))\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params = {'kernel': kernel, 'C': C}\n",
    "\n",
    "print(\"Best parameters based on validation: {}\".format(best_params))\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(C=best_params['C'], kernel=best_params['kernel']))\n",
    "])\n",
    "svm_pipeline.fit(df_train_lr, y_train)\n",
    "\n",
    "train_pred = svm_pipeline.predict(df_train_lr)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "train_f1 = f1_score(y_train, train_pred, average='weighted')\n",
    "print(\"Training Accuracy: {:.4f}, Training F1: {:.4f}\".format(train_acc, train_f1))\n",
    "\n",
    "test_pred = svm_pipeline.predict(df_test_lr)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "print(\"Test Accuracy: {:.4f}, Test F1: {:.4f}\".format(test_acc, test_f1))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_pred, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cabc481-7daa-404b-ab98-649b5f5e04e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0ebde41-6a22-4733-bd7e-76ecb5453dff",
   "metadata": {},
   "source": [
    "Usually SVM works best for this kind of stuff, might overfit a bit though\n",
    "\n",
    "# **testing for random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "caab5bfa-97e9-4c26-8d3d-ffe0b68186a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 100, max_depth: None, Validation Accuracy: 0.8995, Validation F1: 0.8998\n",
      "n_estimators: 100, max_depth: 10, Validation Accuracy: 0.7912, Validation F1: 0.7835\n",
      "n_estimators: 100, max_depth: 20, Validation Accuracy: 0.8171, Validation F1: 0.8118\n",
      "n_estimators: 200, max_depth: None, Validation Accuracy: 0.9002, Validation F1: 0.9005\n",
      "n_estimators: 200, max_depth: 10, Validation Accuracy: 0.7958, Validation F1: 0.7877\n",
      "n_estimators: 200, max_depth: 20, Validation Accuracy: 0.8143, Validation F1: 0.8090\n",
      "Best parameters:  {'n_estimators': 200, 'max_depth': None}\n",
      "Training Accuracy: 0.9884, Training F1: 0.9884\n",
      "Test Accuracy: 0.8378, Test F1: 0.8367\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.87      0.86      2047\n",
      "     neutral       0.79      0.74      0.77      2047\n",
      "    positive       0.86      0.90      0.88      2047\n",
      "\n",
      "    accuracy                           0.84      6141\n",
      "   macro avg       0.84      0.84      0.84      6141\n",
      "weighted avg       0.84      0.84      0.84      6141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_val_acc = 0\n",
    "best_params = {}\n",
    "for n_estimators in [100, 200]:\n",
    "    for max_depth in [None, 10, 20]:\n",
    "        rf_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42))\n",
    "        ])\n",
    "        rf_pipeline.fit(df_train_lr, y_train)\n",
    "        val_pred = rf_pipeline.predict(df_val_lr)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        val_f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "        print(\"n_estimators: {}, max_depth: {}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}\".format(n_estimators, max_depth, val_acc, val_f1))\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_params = {\"n_estimators\": n_estimators, \"max_depth\": max_depth}\n",
    "\n",
    "print(\"Best parameters: \", best_params)\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], random_state=42))\n",
    "])\n",
    "rf_pipeline.fit(df_train_lr, y_train)\n",
    "\n",
    "train_pred = rf_pipeline.predict(df_train_lr)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "train_f1 = f1_score(y_train, train_pred, average='weighted')\n",
    "print(\"Training Accuracy: {:.4f}, Training F1: {:.4f}\".format(train_acc, train_f1))\n",
    "\n",
    "test_pred = rf_pipeline.predict(df_test_lr)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "print(\"Test Accuracy: {:.4f}, Test F1: {:.4f}\".format(test_acc, test_f1))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_pred, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e5bfcf-f5f7-46ec-8e0a-6c7f96422ef0",
   "metadata": {},
   "source": [
    "Too unstable, we aint using this\n",
    "\n",
    "# **testing for Xgboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88d0513a-66d1-4d5f-b4d6-40289c0d5b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singapore Athletic\\Desktop\\Alph\\Sem2\\BT5151\\Sentiment_Analysis\\env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [00:15:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 100, learning_rate: 0.1, max_depth: 3 - Validation Accuracy: 0.8192, Validation F1: 0.8153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singapore Athletic\\Desktop\\Alph\\Sem2\\BT5151\\Sentiment_Analysis\\env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [00:15:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 100, learning_rate: 0.1, max_depth: 5 - Validation Accuracy: 0.8363, Validation F1: 0.8342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singapore Athletic\\Desktop\\Alph\\Sem2\\BT5151\\Sentiment_Analysis\\env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [00:15:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 100, learning_rate: 0.05, max_depth: 3 - Validation Accuracy: 0.7976, Validation F1: 0.7906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singapore Athletic\\Desktop\\Alph\\Sem2\\BT5151\\Sentiment_Analysis\\env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [00:15:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 100, learning_rate: 0.05, max_depth: 5 - Validation Accuracy: 0.8231, Validation F1: 0.8194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singapore Athletic\\Desktop\\Alph\\Sem2\\BT5151\\Sentiment_Analysis\\env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [00:15:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 200, learning_rate: 0.1, max_depth: 3 - Validation Accuracy: 0.8347, Validation F1: 0.8321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singapore Athletic\\Desktop\\Alph\\Sem2\\BT5151\\Sentiment_Analysis\\env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [00:15:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 200, learning_rate: 0.1, max_depth: 5 - Validation Accuracy: 0.8503, Validation F1: 0.8491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singapore Athletic\\Desktop\\Alph\\Sem2\\BT5151\\Sentiment_Analysis\\env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [00:16:11] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 200, learning_rate: 0.05, max_depth: 3 - Validation Accuracy: 0.8187, Validation F1: 0.8148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singapore Athletic\\Desktop\\Alph\\Sem2\\BT5151\\Sentiment_Analysis\\env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [00:16:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 200, learning_rate: 0.05, max_depth: 5 - Validation Accuracy: 0.8336, Validation F1: 0.8313\n",
      "Best parameters based on validation: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Singapore Athletic\\Desktop\\Alph\\Sem2\\BT5151\\Sentiment_Analysis\\env\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [00:16:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8729, Training F1: 0.8716\n",
      "Test Accuracy: 0.8404, Test F1: 0.8390\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.88      0.87      2047\n",
      "     neutral       0.79      0.73      0.76      2047\n",
      "    positive       0.87      0.91      0.89      2047\n",
      "\n",
      "    accuracy                           0.84      6141\n",
      "   macro avg       0.84      0.84      0.84      6141\n",
      "weighted avg       0.84      0.84      0.84      6141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_val_acc = 0\n",
    "best_params = {}\n",
    "for n_estimators in [100, 200]:\n",
    "    for learning_rate in [0.1, 0.05]:\n",
    "        for max_depth in [3, 5]:\n",
    "            gb_pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('classifier', XGBClassifier(\n",
    "                    n_estimators=n_estimators,\n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth,\n",
    "                    use_label_encoder=False,\n",
    "                    eval_metric='mlogloss',\n",
    "                    random_state=42))\n",
    "            ])\n",
    "            gb_pipeline.fit(df_train_lr, y_train)\n",
    "            val_pred = gb_pipeline.predict(df_val_lr)\n",
    "            val_acc = accuracy_score(y_val, val_pred)\n",
    "            val_f1 = f1_score(y_val, val_pred, average='weighted')\n",
    "            print(\"n_estimators: {}, learning_rate: {}, max_depth: {} - Validation Accuracy: {:.4f}, Validation F1: {:.4f}\"\n",
    "                  .format(n_estimators, learning_rate, max_depth, val_acc, val_f1))\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_params = {'n_estimators': n_estimators, 'learning_rate': learning_rate, 'max_depth': max_depth}\n",
    "\n",
    "print(\"Best parameters based on validation:\", best_params)\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=best_params['n_estimators'],\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        max_depth=best_params['max_depth'],\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42))\n",
    "])\n",
    "gb_pipeline.fit(df_train_lr, y_train)\n",
    "\n",
    "train_pred = gb_pipeline.predict(df_train_lr)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "train_f1 = f1_score(y_train, train_pred, average='weighted')\n",
    "print(\"Training Accuracy: {:.4f}, Training F1: {:.4f}\".format(train_acc, train_f1))\n",
    "\n",
    "test_pred = gb_pipeline.predict(df_test_lr)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "test_f1 = f1_score(y_test, test_pred, average='weighted')\n",
    "print(\"Test Accuracy: {:.4f}, Test F1: {:.4f}\".format(test_acc, test_f1))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, test_pred, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dde4c0-108d-4e65-9e2f-20e905014fcf",
   "metadata": {},
   "source": [
    "Huh this works really well and no sign of overfitting, probably best model\n",
    "\n",
    "for these smaller and simpler problems, there is no need for deep learning hence why we dont try any of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9a1d3-8cbc-4769-97f6-a3b75e2f6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42))\n",
    "])\n",
    "final_model.fit(df_train_lr, y_train)\n",
    "\n",
    "final_train_pred = final_model.predict(df_train_lr)\n",
    "final_train_acc = accuracy_score(y_train, final_train_pred)\n",
    "final_test_pred = final_model.predict(df_test_lr)\n",
    "final_test_acc = accuracy_score(y_test, final_test_pred)\n",
    "\n",
    "print(\"Final Model - Training Accuracy: {:.4f}\".format(final_train_acc))\n",
    "print(\"Final Model - Test Accuracy: {:.4f}\".format(final_test_acc))\n",
    "print(\"Final Model - Test Classification Report:\")\n",
    "print(classification_report(y_test, final_test_pred, target_names=['negative', 'neutral', 'positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535464c-9e2d-4633-8af4-6301a36a3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to call\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    \"Rate\": [4.0],\n",
    "    \"Review\": [\"The product exceeded my expectations and works flawlessly.\"],\n",
    "    \"Summary\": [\"Excellent product with superb performance.\"]\n",
    "})\n",
    "\n",
    "predicted_sentiment = final_model.predict(new_data)\n",
    "print(\"Predicted sentiment:\", predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7c05e-f1e3-4921-8720-d5d114e48b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
